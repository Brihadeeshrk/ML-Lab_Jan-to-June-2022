{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HandcodedMLP_and_IamTrask.ipynb","provenance":[],"authorship_tag":"ABX9TyO5GKw6xsSIWdTJPmCROcHx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0r5wa6-8YfN","executionInfo":{"status":"ok","timestamp":1650709368718,"user_tz":-330,"elapsed":2422,"user":{"displayName":"Dr.Mydhili Nair","userId":"00267933281365461047"}},"outputId":"a4ce0c08-4fd7-4b4e-a328-2f4196aeb5c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Stage 1) Random starting synaptic weights: \n","    Layer 1 (4 neurons, each with 3 inputs):\n","[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n"," [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n"," [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n","    Layer 2 (1 neuron, with 4 inputs):\n","[[-0.5910955 ]\n"," [ 0.75623487]\n"," [-0.94522481]\n"," [ 0.34093502]]\n","Stage 2) New synaptic weights after training: \n","    Layer 1 (4 neurons, each with 3 inputs):\n","[[ 0.3122465   4.57704063 -6.15329916 -8.75834924]\n"," [ 0.19676933 -8.74975548 -6.1638187   4.40720501]\n"," [-0.03327074 -0.58272995  0.08319184 -0.39787635]]\n","    Layer 2 (1 neuron, with 4 inputs):\n","[[ -8.18850925]\n"," [ 10.13210706]\n"," [-21.33532796]\n"," [  9.90935111]]\n","Stage 3) Considering a new situation [1, 1, 0] -> ?: \n","[0.0078876]\n"]}],"source":["from numpy import exp, array, random, dot\n","\n","\n","class NeuronLayer():\n","    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n","        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n","\n","\n","class NeuralNetwork():\n","    def __init__(self, layer1, layer2):\n","        self.layer1 = layer1\n","        self.layer2 = layer2\n","\n","    # The Sigmoid function, which describes an S shaped curve.\n","    # We pass the weighted sum of the inputs through this function to\n","    # normalise them between 0 and 1.\n","    def __sigmoid(self, x):\n","        return 1 / (1 + exp(-x))\n","\n","    # The derivative of the Sigmoid function.\n","    # This is the gradient of the Sigmoid curve.\n","    # It indicates how confident we are about the existing weight.\n","    def __sigmoid_derivative(self, x):\n","        return x * (1 - x)\n","\n","    # We train the neural network through a process of trial and error.\n","    # Adjusting the synaptic weights each time.\n","    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n","        for iteration in range(number_of_training_iterations):\n","            # Pass the training set through our neural network\n","            output_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)\n","\n","            # Calculate the error for layer 2 (The difference between the desired output\n","            # and the predicted output).\n","            layer2_error = training_set_outputs - output_from_layer_2\n","            layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\n","\n","            # Calculate the error for layer 1 (By looking at the weights in layer 1,\n","            # we can determine by how much layer 1 contributed to the error in layer 2).\n","            layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n","            layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n","\n","            # Calculate how much to adjust the weights by\n","            layer1_adjustment = training_set_inputs.T.dot(layer1_delta)\n","            layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\n","\n","            # Adjust the weights.\n","            self.layer1.synaptic_weights += layer1_adjustment\n","            self.layer2.synaptic_weights += layer2_adjustment\n","\n","    # The neural network thinks.\n","    def think(self, inputs):\n","        output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))\n","        output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))\n","        return output_from_layer1, output_from_layer2\n","\n","    # The neural network prints its weights\n","    def print_weights(self):\n","        print(\"    Layer 1 (4 neurons, each with 3 inputs):\")\n","        print(self.layer1.synaptic_weights)\n","        print(\"    Layer 2 (1 neuron, with 4 inputs):\")\n","        print(self.layer2.synaptic_weights)\n","\n","if __name__ == \"__main__\":\n","\n","    #Seed the random number generator\n","    random.seed(1)\n","\n","    # Create layer 1 (4 neurons, each with 3 inputs)\n","    layer1 = NeuronLayer(4, 3)\n","\n","    # Create layer 2 (a single neuron with 4 inputs)\n","    layer2 = NeuronLayer(1, 4)\n","\n","    # Combine the layers to create a neural network\n","    neural_network = NeuralNetwork(layer1, layer2)\n","\n","    print(\"Stage 1) Random starting synaptic weights: \")\n","    neural_network.print_weights()\n","\n","    # The training set. We have 7 examples, each consisting of 3 input values\n","    # and 1 output value.\n","    training_set_inputs = array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1], [0, 0, 0]])\n","    training_set_outputs = array([[0, 1, 1, 1, 1, 0, 0]]).T\n","\n","    # Train the neural network using the training set.\n","    # Do it 60,000 times and make small adjustments each time.\n","    neural_network.train(training_set_inputs, training_set_outputs, 60000)\n","\n","    print(\"Stage 2) New synaptic weights after training: \")\n","    neural_network.print_weights()\n","\n","    # Test the neural network with a new situation.\n","    print(\"Stage 3) Considering a new situation [1, 1, 0] -> ?: \")\n","    hidden_state, output = neural_network.think(array([1, 1, 0]))\n","    print(output)"]},{"cell_type":"code","source":["import numpy as np \n","\n","X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n","\n","y = np.array([[0,1,1,0]]).T\n","\n","syn0 = 2*np.random.random((3,4)) - 1\n","\n","syn1 = 2*np.random.random((4,1)) - 1\n","\n","for j in range(60000):\n","\n","  l1 = 1/(1+np.exp(-(np.dot(X,syn0))))\n","\n","  l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))\n","\n","  l2_delta = (y - l2)*(l2*(1-l2))\n","\n","  l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))\n","\n","  syn1 += l1.T.dot(l2_delta)\n","\n","  syn0 += X.T.dot(l1_delta)"],"metadata":{"id":"z_Tcj09IAkj6","executionInfo":{"status":"ok","timestamp":1650710995182,"user_tz":-330,"elapsed":1939,"user":{"displayName":"Dr.Mydhili Nair","userId":"00267933281365461047"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\n","import numpy as np\n","\n","# sigmoid function\n","\n","def nonlin(x,deriv=False):\n","\n","  if(deriv==True):\n","\n","    return x*(1-x)\n","\n","  return 1/(1+np.exp(-x))\n","\n","# input dataset\n","\n","X = np.array([  [0,0,1],\n","\n","[0,1,1],\n","\n","[1,0,1],\n","\n","[1,1,1] ])\n","\n","# output dataset           \n","\n","y = np.array([[0,0,1,1]]).T\n","\n","# seed random numbers to make calculation\n","# deterministic (just a good practice)\n","\n","np.random.seed(1)\n","\n"," \n","\n","# initialize weights randomly with mean 0\n","\n","syn0 = 2*np.random.random((3,1)) - 1\n","\n"," \n","\n","for iter in range(10000):\n","\n"," \n","\n","# forward propagation\n","\n","  l0 = X\n","\n","  l1 = nonlin(np.dot(l0,syn0))\n","\n","# how much did we miss?\n","\n","  l1_error = y - l1\n","\n","# multiply how much we missed by the\n","\n","# slope of the sigmoid at the values in l\n","  l1_delta = l1_error * nonlin(l1,True)\n","\n","# update weights\n","\n","  syn0 += np.dot(l0.T,l1_delta)\n","\n","print (\"Output After Training:\")\n","\n","print (l1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMHdJHl_BwF5","executionInfo":{"status":"ok","timestamp":1650711128645,"user_tz":-330,"elapsed":414,"user":{"displayName":"Dr.Mydhili Nair","userId":"00267933281365461047"}},"outputId":"44f7d2fb-f0c3-4c62-b522-f8c23fbe2f9e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Output After Training:\n","[[0.00966449]\n"," [0.00786506]\n"," [0.99358898]\n"," [0.99211957]]\n"]}]}]}